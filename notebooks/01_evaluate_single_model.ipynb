{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8467ac63",
   "metadata": {},
   "source": [
    "Basic Evaluation Workflow\n",
    "==========================\n",
    "This notebook demonstrates evaluating a single downscaled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf10c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClimatExEval loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from climate_eval import io, metrics, plotting\n",
    "from climate_eval.core import EvaluationResults\n",
    "from climate_eval.utils import standardize_dimension_names\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"ClimatExEval loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e990dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading configuration from ../config.yaml\n",
      "ðŸ“Š Dataset configured: /home/sbeairsto/data-sbeairsto/van_isle_128/trained_generators/hr_inf_generator_bs_64_ns_6_lr_00025_drif_pen_uas.zarr (zarr)\n",
      "ðŸ“Š Dataset configured: /home/sbeairsto/data-sbeairsto/van_isle_128/uas_validation_hr.zarr (zarr)\n",
      "ðŸ“ˆ Metrics configured: 4 total\n",
      "âš¡ Dask enabled with 4 workers\n",
      "ðŸ’¾ Output directory: results\n",
      "ðŸŒ Evaluation project: uas_evaluation\n",
      "   Evaluation of GAN-downscaled uas winds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project: uas_evaluation\n",
      "Description: Evaluation of GAN-downscaled uas winds\n",
      "Metrics to compute: ['bias', 'quantile_comparison', 'spatial_correlation', 'temporal_autocorrelation']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from YAML file.\n",
    "\n",
    "config = io.load_config('../config.yaml')\n",
    "print(f\"\\nProject: {config.project_name}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"Metrics to compute: {config.metrics.all_metrics()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989f60d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Found 5 realizations\n",
      "   Selecting first realization\n",
      "ðŸ“¥ Loaded predicted data: {'time': 8760, 'rlon': 128, 'rlat': 128}\n",
      "   No 'realization' dimension found\n",
      "ðŸ“¥ Loaded reference data: {'time': 8760, 'rlat': 128, 'rlon': 128}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted: LoadedDataset(type=predicted, variables=['uas'], shape={'time': 8760, 'rlon': 128, 'rlat': 128})\n",
      "Reference: LoadedDataset(type=reference, variables=['uas'], shape={'time': 8760, 'rlat': 128, 'rlon': 128})\n"
     ]
    }
   ],
   "source": [
    "# Load predicted and reference datasets.\n",
    "\n",
    "pred_dataset = io.load_dataset(config.data_predicted, dataset_type=\"predicted\")\n",
    "ref_dataset = io.load_dataset(config.data_reference, dataset_type=\"reference\")\n",
    "\n",
    "print(f\"\\nPredicted: {pred_dataset}\")\n",
    "print(f\"Reference: {ref_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0accfe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renaming dimensions: {'rlat': 'lat', 'rlon': 'lon'}\n",
      "Renaming dimensions: {'rlat': 'lat', 'rlon': 'lon'}\n",
      "ðŸ”„ Aligning datasets...\n",
      "   Renamed variables: {'uas': 'uas'}\n",
      "   Common variables: ['uas']\n",
      "   Grids already aligned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "Predicted: {'time': 8760, 'lon': 128, 'lat': 128}\n",
      "Reference: {'time': 8760, 'lat': 128, 'lon': 128}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "\n",
    "# Standardize dimension names (rlat/rlon -> lat/lon)\n",
    "pred_dataset.data = standardize_dimension_names(pred_dataset.data)\n",
    "ref_dataset.data = standardize_dimension_names(ref_dataset.data)\n",
    "\n",
    "# Subset to domain of interest\n",
    "pred_dataset.data = io.subset_domain(pred_dataset.data, config.domain)\n",
    "ref_dataset.data = io.subset_domain(ref_dataset.data, config.domain)\n",
    "\n",
    "# Align grids and variable names\n",
    "pred_dataset, ref_dataset = io.align_datasets(pred_dataset, ref_dataset)\n",
    "\n",
    "print(f\"\\nAfter preprocessing:\")\n",
    "print(f\"Predicted: {dict(pred_dataset.data.sizes)}\")\n",
    "print(f\"Reference: {dict(ref_dataset.data.sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ec5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results container initialized for: uas_evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create results container for storing all metrics.\n",
    "\n",
    "results = EvaluationResults(config=config)\n",
    "print(f\"\\nResults container initialized for: {results.config.project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27948b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating variable: uas\n",
      "Shape: (8760, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Choose variable to evaluate.\n",
    "\n",
    "# Change this to evaluate different variables\n",
    "variable = 'uas'\n",
    "\n",
    "pred_var = pred_dataset[variable]\n",
    "ref_var = ref_dataset[variable]\n",
    "\n",
    "print(f\"\\nEvaluating variable: {variable}\")\n",
    "print(f\"Shape: {pred_var.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3703cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbeairsto/projects/ClimatExEval/.eval_venv/lib/python3.12/site-packages/dask/array/core.py:5198: PerformanceWarning: Increasing number of chunks by factor of 11\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MARGINAL METRICS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âœ… Computed metric: uas_bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean bias: 0.5665\n"
     ]
    }
   ],
   "source": [
    "# Compute mean bias field.\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MARGINAL METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute bias (time mean)\n",
    "bias_field = metrics.bias(pred_var, ref_var, dim='time')\n",
    "results.add_result(f'{variable}_bias', bias_field)\n",
    "print(f\"Mean bias: {float(bias_field.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf82e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âœ… Computed metric: uas_quantiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles computed: [np.float64(0.01), np.float64(0.05), np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.95), np.float64(0.99)]\n"
     ]
    }
   ],
   "source": [
    "# Compare quantile distributions.\n",
    "\n",
    "quantile_results = metrics.quantile_comparison(pred_var, ref_var)  # Renamed variable\n",
    "results.add_result(f'{variable}_quantiles', quantile_results)\n",
    "print(f\"Quantiles computed: {list(quantile_results.coords['quantile'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926a110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SPATIAL METRICS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbeairsto/projects/ClimatExEval/.eval_venv/lib/python3.12/site-packages/dask/array/core.py:5198: PerformanceWarning: Increasing number of chunks by factor of 11\n",
      "  result = blockwise(\n",
      "âœ… Computed metric: uas_spatial_correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean spatial correlation: 0.2805\n"
     ]
    }
   ],
   "source": [
    "# Compute spatial correlation field.\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPATIAL METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "spatial_corr = metrics.spatial_correlation(pred_var, ref_var)\n",
    "results.add_result(f'{variable}_spatial_correlation', spatial_corr)\n",
    "print(f\"Mean spatial correlation: {float(spatial_corr.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13193410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Loading data into memory for ACF computation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPORAL METRICS\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute temporal autocorrelation.\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute ACF for predicted data\n",
    "pred_acf = metrics.temporal_autocorrelation(pred_var, max_lag=30)\n",
    "results.add_result(f'{variable}_pred_acf', pred_acf)\n",
    "\n",
    "# Compute ACF for reference data\n",
    "ref_acf = metrics.temporal_autocorrelation(ref_var, max_lag=30)\n",
    "results.add_result(f'{variable}_ref_acf', ref_acf)\n",
    "\n",
    "print(f\"ACF computed with {len(pred_acf.lag)} lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3605ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution comparison.\n",
    "\n",
    "fig, ax = plotting.distributions.plot_histogram_comparison(\n",
    "    pred_var, ref_var,\n",
    "    title=f'{variable.title()} Distribution Comparison',\n",
    "    bins=50\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d17d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max value for color scale\n",
    "bias_max = float(abs(bias_field).max().compute())  # Added .compute()\n",
    "\n",
    "fig, ax = plotting.maps.plot_spatial_field(\n",
    "    bias_field,\n",
    "    title=f'{variable.title()} Time-Mean Bias',\n",
    "    cmap='RdBu_r',\n",
    "    vmin=-bias_max,\n",
    "    vmax=bias_max\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare temporal autocorrelation.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot predicted ACF\n",
    "plt.subplot(1, 2, 1)\n",
    "pred_acf.mean(dim=['lat', 'lon']).plot(marker='o', label='Predicted')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Lag (days)')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Predicted ACF')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot reference ACF\n",
    "plt.subplot(1, 2, 2)\n",
    "ref_acf.mean(dim=['lat', 'lon']).plot(marker='s', label='Reference', color='C1')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Lag (days)')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Reference ACF')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of all scalar metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "summary = results.summary()\n",
    "if summary:\n",
    "    for metric_name, value in summary.items():\n",
    "        print(f\"{metric_name}: {value:.6f}\")\n",
    "else:\n",
    "    print(\"No scalar metrics computed.\")\n",
    "\n",
    "print(f\"\\nTotal metrics computed: {len(results.results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to disk.\n",
    "\n",
    "io.save_results(results, name=variable)\n",
    "print(f\"\\nâœ… Results saved to: {config.output.dir / config.project_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".eval_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
